{"cells":[{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T05:44:25.672722Z","iopub.status.busy":"2024-07-04T05:44:25.671798Z","iopub.status.idle":"2024-07-04T05:44:25.681301Z","shell.execute_reply":"2024-07-04T05:44:25.680286Z","shell.execute_reply.started":"2024-07-04T05:44:25.672685Z"},"id":"w-H7jVy6TZjG","trusted":true},"outputs":[],"source":["# import\n","\n","import numpy as np\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision import datasets\n","import matplotlib.pyplot as plt\n","import torchinfo\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","from pytz import timezone"]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T05:44:26.789598Z","iopub.status.busy":"2024-07-04T05:44:26.788983Z","iopub.status.idle":"2024-07-04T05:44:26.794821Z","shell.execute_reply":"2024-07-04T05:44:26.793731Z","shell.execute_reply.started":"2024-07-04T05:44:26.789566Z"},"id":"OLzGmT0HTZjJ","trusted":true},"outputs":[],"source":["# 하이퍼파라미터 설정\n","RANDOM_SEED = 4242\n","LEARNING_RATE = 0.01\n","BATCH_SIZE = 32\n","EPOCHS = 90\n","IMG_SIZE = 227\n","NUM_CLASSES = 1000\n","\n","# 규제화 파라미터 설정\n","DROPOUT = 0.5\n","LRN_K = 2.0\n","LRN_ALPHA = 0.0001\n","LRN_BETA = 0.75\n","LRN_N = 5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# AlexNet 현대적으로 재해석한 책에서 제시된 버전으로 구현 (multi-GPU 구현하지 않음, 배치정규화 차용)\n","class ModernAlexNet(nn.Module):\n","    def __init__(self):\n","        super(ModernAlexNet, self).__init__()\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0)\n","        self.batchnorm1 = nn.BatchNorm2d(96)\n","        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n","        self.batchnorm2 = nn.BatchNorm2d(256)\n","        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n","        self.batchnorm3 = nn.BatchNorm2d(384)\n","        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n","        self.batchnorm4 = nn.BatchNorm2d(384)\n","        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n","        self.batchnorm5 = nn.BatchNorm2d(256)\n","        self.fc6 = nn.Linear(6 * 6 * 256, 4096)\n","        self.fc7 = nn.Linear(4096, 4096)\n","        self.fc8 = nn.Linear(4096, NUM_CLASSES)\n","\n","    def forward(x, self):\n","        # Conv 1\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = self.batchnorm1(x)\n","        # Conv 2\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.batchnorm2(x)\n","        # Conv 3\n","        x = F.relu(self.conv3(x))\n","        x = self.batchnorm3(x)\n","        # Conv 4\n","        x = F.relu(self.conv4(x))\n","        x = self.batchnorm4(x)\n","        # Conv 5\n","        x = F.relu(self.conv5(x))\n","        x = self.batchnorm5(x)\n","        x = self.pool(x)\n","        # FC 6\n","        x = x.view(-1, 6 * 6 * 256)\n","        x = F.relu(self.fc6(x))\n","        x = self.dropout(x)\n","        # FC 7\n","        x = F.relu(self.fc7(x))\n","        x = self.dropout(x)\n","        # FC 8\n","        logits = self.fc8(x)\n","        return logits\n","\n","torchinfo.summary(\n","    ModernAlexNet(),\n","    input_size=(1, 3, IMG_SIZE, IMG_SIZE),\n","    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n","    row_settings=[\"depth\", \"var_names\"],\n",")"]},{"cell_type":"code","execution_count":149,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-07-04T05:44:27.540744Z","iopub.status.busy":"2024-07-04T05:44:27.540365Z","iopub.status.idle":"2024-07-04T05:44:28.242548Z","shell.execute_reply":"2024-07-04T05:44:28.241582Z","shell.execute_reply.started":"2024-07-04T05:44:27.540716Z"},"id":"3PdseEJ1TZjL","outputId":"0b7137bf-012e-40bf-a445-29adcb7a2d90","trusted":true},"outputs":[{"data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n","============================================================================================================================================\n","AlexNet (AlexNet)                        [1, 3, 227, 227]          [1, 1000]                 --                        --\n","├─Conv2d (conv1_a): 1-1                  [1, 3, 227, 227]          [1, 48, 55, 55]           17,472                    [11, 11]\n","├─LocalResponseNorm (lrn): 1-2           [1, 48, 55, 55]           [1, 48, 55, 55]           --                        --\n","├─Conv2d (conv1_b): 1-3                  [1, 3, 227, 227]          [1, 48, 55, 55]           17,472                    [11, 11]\n","├─LocalResponseNorm (lrn): 1-4           [1, 48, 55, 55]           [1, 48, 55, 55]           --                        --\n","├─MaxPool2d (pool1): 1-5                 [1, 48, 55, 55]           [1, 48, 27, 27]           --                        3\n","├─MaxPool2d (pool1): 1-6                 [1, 48, 55, 55]           [1, 48, 27, 27]           --                        3\n","├─Conv2d (conv2_a): 1-7                  [1, 48, 27, 27]           [1, 128, 27, 27]          153,728                   [5, 5]\n","├─LocalResponseNorm (lrn): 1-8           [1, 128, 27, 27]          [1, 128, 27, 27]          --                        --\n","├─Conv2d (conv2_b): 1-9                  [1, 48, 27, 27]           [1, 128, 27, 27]          153,728                   [5, 5]\n","├─LocalResponseNorm (lrn): 1-10          [1, 128, 27, 27]          [1, 128, 27, 27]          --                        --\n","├─MaxPool2d (pool2): 1-11                [1, 128, 27, 27]          [1, 128, 13, 13]          --                        3\n","├─MaxPool2d (pool2): 1-12                [1, 128, 27, 27]          [1, 128, 13, 13]          --                        3\n","├─Conv2d (conv3_a): 1-13                 [1, 256, 13, 13]          [1, 192, 13, 13]          442,560                   [3, 3]\n","├─Conv2d (conv3_b): 1-14                 [1, 256, 13, 13]          [1, 192, 13, 13]          442,560                   [3, 3]\n","├─Conv2d (conv4_a): 1-15                 [1, 192, 13, 13]          [1, 192, 13, 13]          331,968                   [3, 3]\n","├─Conv2d (conv4_b): 1-16                 [1, 192, 13, 13]          [1, 192, 13, 13]          331,968                   [3, 3]\n","├─Conv2d (conv5_a): 1-17                 [1, 192, 13, 13]          [1, 128, 13, 13]          221,312                   [3, 3]\n","├─Conv2d (conv5_b): 1-18                 [1, 192, 13, 13]          [1, 128, 13, 13]          221,312                   [3, 3]\n","├─MaxPool2d (pool5): 1-19                [1, 128, 13, 13]          [1, 128, 6, 6]            --                        3\n","├─MaxPool2d (pool5): 1-20                [1, 128, 13, 13]          [1, 128, 6, 6]            --                        3\n","├─Linear (fc6_a): 1-21                   [1, 9216]                 [1, 2048]                 18,876,416                --\n","├─Dropout (dropout): 1-22                [1, 2048]                 [1, 2048]                 --                        --\n","├─Linear (fc6_b): 1-23                   [1, 9216]                 [1, 2048]                 18,876,416                --\n","├─Dropout (dropout): 1-24                [1, 2048]                 [1, 2048]                 --                        --\n","├─Linear (fc7_a): 1-25                   [1, 4096]                 [1, 2048]                 8,390,656                 --\n","├─Dropout (dropout): 1-26                [1, 2048]                 [1, 2048]                 --                        --\n","├─Linear (fc7_b): 1-27                   [1, 4096]                 [1, 2048]                 8,390,656                 --\n","├─Dropout (dropout): 1-28                [1, 2048]                 [1, 2048]                 --                        --\n","├─Linear (fc8): 1-29                     [1, 4096]                 [1, 1000]                 4,097,000                 --\n","============================================================================================================================================\n","Total params: 60,965,224\n","Trainable params: 60,965,224\n","Non-trainable params: 0\n","Total mult-adds (M): 725.07\n","============================================================================================================================================\n","Input size (MB): 0.62\n","Forward/backward pass size (MB): 5.27\n","Params size (MB): 243.86\n","Estimated Total Size (MB): 249.75\n","============================================================================================================================================"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["# AlexNet 논문과 최대한 유사하게 구현 (multi-GPU를 표현만 하고, 실제 구현하지 않음)\n","class PseudoAlexNet(nn.Module):\n","    def __init__(self):\n","        super(PseudoAlexNet, self).__init__()\n","        self.lrn = nn.LocalResponseNorm(LRN_N, alpha=LRN_ALPHA, beta=LRN_BETA, k=LRN_K)\n","        self.dropout = nn.Dropout(DROPOUT)\n","\n","        self.conv1_a = nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=0)\n","        self.conv1_b = nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=0)\n","        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        self.conv2_a = nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2)\n","        self.conv2_b = nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2)\n","        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        self.conv3_a = nn.Conv2d(128 * 2, 192, kernel_size=3, stride=1, padding=1)\n","        self.conv3_b = nn.Conv2d(128 * 2, 192, kernel_size=3, stride=1, padding=1)\n","\n","        self.conv4_a = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1)\n","        self.conv4_b = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1)\n","\n","        self.conv5_a = nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1)\n","        self.conv5_b = nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1)\n","        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n","\n","        self.fc6_a = nn.Linear(128 * 6 * 6 * 2, 2048)\n","        self.fc6_b = nn.Linear(128 * 6 * 6 * 2, 2048)\n","\n","        self.fc7_a = nn.Linear(2048 * 2, 2048)\n","        self.fc7_b = nn.Linear(2048 * 2, 2048)\n","\n","        self.fc8 = nn.Linear(2048 * 2, NUM_CLASSES)\n","\n","    def forward(self, x):\n","        # Conv 1\n","        x_a = self.lrn(F.relu(self.conv1_a(x)))\n","        x_b = self.lrn(F.relu(self.conv1_b(x)))\n","        x_a = self.pool1(x_a)\n","        x_b = self.pool1(x_b)\n","        # Conv 2\n","        x_a = self.lrn(F.relu(self.conv2_a(x_a)))\n","        x_b = self.lrn(F.relu(self.conv2_b(x_b)))\n","        x_a = self.pool2(x_a)\n","        x_b = self.pool2(x_b)\n","        # Conv 3, GPU 데이터 합치고 각각 연산 수행\n","        x = torch.cat((x_a, x_b), dim=1)\n","        x_a = F.relu(self.conv3_a(x))\n","        x_b = F.relu(self.conv3_b(x))\n","        # Conv 4\n","        x_a = F.relu(self.conv4_a(x_a))\n","        x_b = F.relu(self.conv4_b(x_b))\n","        # Conv 5\n","        x_a = F.relu(self.conv5_a(x_a))\n","        x_b = F.relu(self.conv5_b(x_b))\n","        x_a = self.pool5(x_a)\n","        x_b = self.pool5(x_b)\n","        # FC 6, GPU 데이터 합치고 각각 연산 수행\n","        x = torch.cat((x_a, x_b), dim=1)\n","        x = x.view(x.size(0), -1)\n","        x_a = self.dropout(F.relu(self.fc6_a(x)))\n","        x_b = self.dropout(F.relu(self.fc6_b(x)))\n","        # FC 7, GPU 데이터 합치고 각각 연산 수행\n","        x = torch.cat((x_a, x_b), dim=1)\n","        x_a = self.dropout(F.relu(self.fc7_a(x)))\n","        x_b = self.dropout(F.relu(self.fc7_b(x)))\n","        # FC 8, GPU 데이터 합쳐서 최종 연산\n","        x = torch.cat((x_a, x_b), dim=1)\n","        logits = self.fc8(x)\n","\n","        return logits\n","\n","\n","torchinfo.summary(\n","    PseudoAlexNet(),\n","    input_size=(1, 3, IMG_SIZE, IMG_SIZE),\n","    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n","    row_settings=[\"depth\", \"var_names\"],\n",")"]},{"cell_type":"code","execution_count":150,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T05:44:30.941505Z","iopub.status.busy":"2024-07-04T05:44:30.941077Z","iopub.status.idle":"2024-07-04T05:44:30.946964Z","shell.execute_reply":"2024-07-04T05:44:30.945911Z","shell.execute_reply.started":"2024-07-04T05:44:30.941450Z"},"trusted":true},"outputs":[],"source":["# # AlexNet 2개 GPU로 나누는 연습\n","# class AlexNet(nn.Module):\n","#     def __init__(self):\n","#         super(AlexNet, self).__init__()\n","        \n","#         self.conv1_a = nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=0).to('cuda:0')\n","#         self.conv1_b = nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=0).to('cuda:1')\n","#         self.pool1_a = nn.MaxPool2d(kernel_size=4).to('cuda:0')\n","#         self.pool1_b = nn.MaxPool2d(kernel_size=4).to('cuda:1')\n","        \n","#         self.fc2 = nn.Linear(16224, NUM_CLASSES).to('cuda:0')\n","\n","#     def forward(self, x):\n","\n","#         x_a = x.to('cuda:0')\n","#         x_b = x.to('cuda:1')\n","#         x_a = F.relu(self.conv1_a(x_a))\n","#         x_b = F.relu(self.conv1_b(x_b))\n","#         x_a = self.pool1_a(x_a)\n","#         x_b = self.pool1_b(x_b)\n","        \n","#         print(\"2nd \", x_a.device)\n","#         print(\"2nd \", x_b.device)\n","        \n","#         x_b = x_b.to('cuda:0')\n","\n","#         x = torch.cat((x_a, x_b), dim=1)\n","#         x = x.view(x.size(0), -1)\n","#         logits = self.fc2(x)\n","\n","#         return logits"]},{"cell_type":"code","execution_count":151,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T05:44:31.874444Z","iopub.status.busy":"2024-07-04T05:44:31.873777Z","iopub.status.idle":"2024-07-04T05:44:32.996561Z","shell.execute_reply":"2024-07-04T05:44:32.995448Z","shell.execute_reply.started":"2024-07-04T05:44:31.874412Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jul  4 05:44:32 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   73C    P0             30W /   70W |    3009MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   77C    P0             31W /   70W |     529MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","Device: cuda\n","Current cuda device: 0\n","Count of using GPUs: 2\n"]}],"source":["!nvidia-smi\n","\n","import os\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:', device)\n","print('Current cuda device:', torch.cuda.current_device())\n","print('Count of using GPUs:', torch.cuda.device_count())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T05:44:34.952255Z","iopub.status.busy":"2024-07-04T05:44:34.951118Z","iopub.status.idle":"2024-07-04T05:44:35.664369Z","shell.execute_reply":"2024-07-04T05:44:35.663450Z","shell.execute_reply.started":"2024-07-04T05:44:34.952208Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Conv1  cuda:0\n","Conv1  cuda:1\n","Conv2  cuda:0\n","Conv2  cuda:1\n","Conv3  cuda:0\n","Conv3  cuda:1\n","Conv4  cuda:0\n","Conv4  cuda:1\n","Conv5  cuda:0\n","Conv5  cuda:1\n","FC6  cuda:0\n","FC6  cuda:1\n","FC7  cuda:0\n","FC7  cuda:1\n","FC8  cuda:0\n","tensor([[-1.3970e-02, -1.3960e-02,  6.6987e-03,  4.7104e-03, -1.1408e-02,\n","         -6.2351e-03, -5.7788e-03,  1.1038e-02,  4.0766e-03,  1.1278e-02,\n","          1.8552e-02, -6.8858e-03,  1.2358e-02, -2.5827e-02, -7.5963e-03,\n","         -1.6843e-03,  6.3736e-04,  8.5741e-03,  9.3615e-03, -2.6996e-03,\n","         -3.6860e-03, -1.5061e-02, -1.0182e-02,  1.4288e-02,  7.3025e-03,\n","         -6.8980e-05, -1.6342e-02, -9.9617e-03,  1.4559e-02, -1.1932e-02,\n","         -3.9111e-03, -1.8996e-02,  1.5565e-02, -5.2459e-03, -1.8285e-02,\n","         -3.8742e-03,  6.8766e-04, -1.4916e-02, -1.4892e-02, -1.8956e-02,\n","          1.7696e-02, -6.9104e-03, -2.3624e-03,  1.0591e-02,  1.3595e-02,\n","          7.1004e-03, -1.1271e-02,  1.5810e-02,  5.0691e-03, -8.0459e-03,\n","         -3.4708e-03,  8.3902e-03,  8.6477e-03, -1.1970e-02,  1.0668e-02,\n","         -6.1132e-03,  2.1134e-02,  1.4664e-03,  2.2666e-04,  3.0716e-03,\n","          7.1160e-03, -1.2978e-02,  7.3423e-03,  6.1761e-03, -2.6006e-03,\n","          1.6589e-02, -4.2399e-03, -2.1417e-02,  1.5859e-04, -3.7986e-03,\n","         -5.5418e-04,  1.2438e-03, -5.4853e-03, -5.0163e-03,  9.8741e-03,\n","          1.9314e-02,  1.4541e-03,  1.0200e-02,  1.1671e-02, -1.7736e-04,\n","          1.4190e-02,  6.4257e-03,  1.7119e-02,  2.8733e-03,  1.8641e-03,\n","          2.2649e-02, -1.5363e-03,  2.6359e-03,  2.3734e-02,  2.2561e-02,\n","          9.5207e-03,  2.0415e-03, -2.2781e-03,  8.8551e-03, -1.5467e-02,\n","         -7.5294e-03,  8.5610e-03, -8.2161e-03, -3.8676e-04, -2.4284e-05,\n","          1.5366e-03, -1.0187e-02,  1.4935e-02, -6.0828e-04, -1.8999e-02,\n","         -1.1234e-02, -3.9231e-03, -2.5313e-04, -3.0716e-03, -1.1652e-03,\n","          1.6056e-02, -1.2319e-02, -1.2628e-02,  1.5887e-02,  5.1357e-03,\n","         -9.9131e-03, -1.1063e-02,  9.3160e-03,  1.7386e-02, -2.2412e-03,\n","          9.7526e-03,  8.4176e-03,  1.3630e-02,  7.6938e-03,  2.0677e-02,\n","         -1.0031e-02,  1.0790e-02,  1.5118e-02, -1.0780e-02, -8.7123e-03,\n","         -7.9390e-04, -4.8668e-04,  2.8211e-03, -1.6941e-02, -2.1987e-03,\n","         -8.5940e-04, -1.2102e-02, -1.0650e-02, -1.4520e-02, -5.0239e-03,\n","          2.3495e-03,  2.9249e-03,  1.2148e-02,  1.7562e-02,  3.0498e-03,\n","         -1.4400e-02, -1.3366e-02,  1.4116e-03, -3.7229e-03, -1.0042e-02,\n","         -3.0974e-03,  5.4961e-03, -9.2623e-03,  1.5563e-02, -1.2937e-02,\n","          7.2510e-03, -1.1139e-02,  6.2669e-03,  5.6380e-03,  5.9519e-03,\n","         -2.1606e-02, -3.0047e-04, -1.0868e-02,  9.6617e-03,  4.9532e-03,\n","         -2.1466e-03,  8.8048e-03,  2.4694e-03, -2.4861e-03,  5.1656e-03,\n","          9.1625e-03, -9.7842e-03, -3.7644e-03, -3.1300e-03, -7.6081e-03,\n","         -1.3470e-02, -3.6792e-03,  9.8402e-03,  1.5520e-02, -1.2407e-02,\n","         -9.2217e-03, -6.1253e-03, -4.1840e-03, -1.8762e-02, -1.4828e-02,\n","          1.4230e-02, -6.0549e-03,  7.6450e-03,  6.0726e-03, -3.7317e-03,\n","         -1.7093e-02, -5.3563e-03,  1.1823e-02, -1.0205e-02,  1.1075e-02,\n","         -1.9211e-02, -1.9038e-02,  4.0731e-03, -1.1926e-03, -4.1241e-03,\n","          2.9623e-03,  1.4613e-02,  1.0885e-02, -1.8809e-03,  1.8296e-02,\n","         -1.0763e-02, -1.5279e-02,  1.1819e-02, -9.7086e-03, -1.0304e-02,\n","         -5.9888e-03,  2.1490e-03,  1.7711e-02, -1.0472e-03,  1.4182e-02,\n","         -1.3710e-04, -2.6566e-03, -1.8019e-02,  7.1795e-03, -1.2050e-02,\n","          1.4326e-02, -1.0534e-02,  5.9769e-03,  1.8131e-03,  2.3929e-03,\n","          4.2077e-03, -4.2669e-04,  5.8421e-03,  1.0682e-02,  6.8376e-03,\n","          2.9628e-03, -7.9089e-03,  1.7399e-02, -1.9527e-02,  1.4810e-02,\n","          1.0553e-02,  1.4445e-03,  5.6123e-03, -2.2534e-02, -4.8753e-03,\n","         -4.5736e-03,  2.0296e-02, -4.0686e-03, -7.6839e-04,  1.1923e-02,\n","         -1.6721e-02, -5.9237e-03, -8.7023e-03, -1.3343e-02,  9.5541e-03,\n","          8.7254e-03,  2.8719e-02, -1.9385e-02, -1.4188e-02,  9.6529e-03,\n","          1.1219e-02,  6.3490e-04, -9.6256e-03, -1.0791e-02, -6.2703e-03,\n","         -3.4404e-03,  2.5744e-03, -1.6592e-02,  1.6278e-02, -5.6817e-03,\n","         -9.6628e-03,  1.7271e-02, -9.1548e-03,  3.9289e-03, -2.3551e-03,\n","          1.3001e-03, -9.1263e-03, -3.9710e-03,  1.8470e-03, -2.3359e-02,\n","         -8.9919e-03, -3.8551e-03,  1.4387e-02, -9.9654e-03,  8.2210e-03,\n","          1.0549e-02, -2.0926e-03, -1.7821e-02, -1.5863e-02,  1.1217e-03,\n","          9.9399e-03, -1.0214e-02,  1.2065e-02,  1.0647e-02, -1.6600e-03,\n","         -6.1152e-03,  1.5434e-03, -1.5391e-02, -5.2147e-03,  1.7927e-02,\n","          6.3625e-03, -1.6912e-02,  1.8565e-03, -7.2271e-03,  1.8387e-03,\n","          1.9353e-02,  3.4487e-03,  6.4600e-03, -6.7546e-03, -1.0455e-02,\n","          6.7538e-03,  1.8945e-02,  3.1497e-03,  1.7070e-02,  1.4168e-02,\n","         -8.3469e-04, -1.6687e-03, -1.0329e-02,  2.8335e-03,  1.4675e-03,\n","          1.0348e-02, -1.1233e-03, -5.1239e-03, -1.3132e-02,  8.9436e-03,\n","         -1.4567e-03, -3.4952e-03, -9.1779e-03,  1.0717e-02,  1.4679e-02,\n","         -8.6070e-03, -2.0206e-02, -1.5972e-03, -6.1956e-03, -6.1811e-03,\n","          8.7515e-03,  1.7130e-02,  1.4213e-02,  4.6662e-03,  1.4170e-03,\n","          1.7346e-02,  8.6605e-03, -6.5028e-04,  1.7831e-03,  1.3931e-02,\n","          1.8042e-02, -5.1664e-03,  5.9077e-03,  1.5954e-02, -1.5771e-02,\n","         -4.1549e-03,  5.1067e-03, -8.6814e-03,  6.0236e-03, -2.9787e-03,\n","          1.6588e-03,  5.8265e-03, -1.2628e-02, -7.7797e-03, -4.8391e-04,\n","         -1.5148e-02,  1.3610e-02, -5.9194e-03, -6.5508e-03,  1.9034e-03,\n","         -4.6051e-03, -3.7511e-03,  6.7883e-03,  1.8542e-02, -6.7614e-03,\n","          7.6421e-03, -8.7040e-03,  1.4928e-02, -2.1328e-02,  6.6899e-03,\n","         -1.2060e-02, -2.7028e-03, -4.8198e-03,  8.9364e-04, -5.6751e-03,\n","         -1.6954e-02,  3.5096e-03, -8.7169e-03, -5.1557e-03, -1.5454e-03,\n","          3.9724e-03, -8.5882e-03, -2.4147e-03,  7.6367e-03, -2.2652e-03,\n","          8.6927e-03, -1.0867e-02,  5.9415e-03, -4.1035e-03, -1.4279e-03,\n","         -1.0362e-02, -1.9632e-03,  3.7017e-03,  1.7289e-02, -6.4898e-03,\n","         -1.8554e-02,  3.0026e-03,  1.5886e-02,  9.0245e-03, -5.6916e-04,\n","          2.5197e-02, -1.5614e-03, -3.1970e-03, -2.1501e-03,  8.2897e-03,\n","          1.3033e-02,  5.0989e-03,  1.3477e-02,  1.0280e-04, -1.8616e-02,\n","         -1.6059e-02, -8.0220e-03, -1.6235e-02,  1.5504e-02,  3.8869e-03,\n","         -1.0082e-02, -1.1122e-02,  1.0562e-02, -1.0693e-02,  1.9689e-02,\n","         -1.7459e-02, -1.9573e-02,  8.1591e-03, -5.3810e-03, -2.0631e-02,\n","         -1.2811e-02, -1.0150e-02,  9.9118e-03, -8.3174e-03, -2.8898e-02,\n","          4.2860e-03,  1.2277e-04,  1.4346e-02, -2.1746e-02,  1.0410e-02,\n","          4.2406e-03,  1.5213e-02,  1.3069e-02, -5.4012e-03, -2.1899e-02,\n","         -1.4857e-02,  3.7662e-03,  1.9234e-02, -5.5367e-04,  8.2047e-03,\n","          4.3172e-03, -1.8929e-02,  1.2978e-02, -1.0178e-02, -1.4599e-02,\n","         -2.7036e-03,  1.2494e-02,  4.7346e-03, -1.0298e-02, -1.0551e-02,\n","          6.5652e-03, -1.0024e-02,  1.6387e-02,  2.0299e-02,  7.7292e-03,\n","          4.2496e-03,  1.9614e-02,  2.5344e-03,  4.3385e-03,  5.2440e-03,\n","          1.4147e-02,  3.8308e-03, -8.8714e-03,  2.1601e-02,  9.9122e-03,\n","         -4.9098e-03,  8.4416e-03,  3.1886e-04,  8.2891e-03,  5.5274e-03,\n","          8.6566e-04,  4.3856e-03,  1.1015e-02, -5.7617e-03, -1.2665e-02,\n","          2.2318e-02,  1.0638e-02,  7.6937e-05,  2.1416e-02,  6.9495e-03,\n","         -1.6293e-02, -9.6629e-03,  5.8869e-03, -9.9773e-03,  2.2338e-03,\n","         -4.1538e-03,  1.7021e-02, -8.2065e-04, -9.9070e-03, -8.8308e-04,\n","          4.9761e-04, -8.9172e-03, -2.4271e-03, -7.9439e-03,  1.4117e-02,\n","          1.5163e-02, -7.0753e-03, -4.8731e-04, -2.6242e-04, -9.1925e-03,\n","          8.7218e-03, -2.2544e-03, -2.9841e-03,  1.2209e-02, -9.0120e-03,\n","          3.8076e-03, -1.1787e-02,  1.2886e-02, -1.2245e-02, -2.4865e-02,\n","         -3.6013e-04, -1.5968e-03,  5.2410e-03,  2.0801e-02,  7.6479e-03,\n","         -2.5972e-02, -9.0261e-04,  8.5333e-03,  1.1676e-03, -8.7867e-03,\n","         -5.6512e-03,  6.9940e-03, -1.5104e-02, -1.8241e-02,  1.4714e-02,\n","         -1.1202e-02,  8.9472e-03, -2.4277e-03,  3.2751e-03,  6.6977e-03,\n","          5.4568e-03,  1.9609e-04,  1.4864e-03,  7.6606e-03, -1.0338e-03,\n","          8.7057e-03,  2.7537e-03, -6.8324e-03,  9.9201e-03, -5.2870e-03,\n","         -1.2146e-02, -6.5423e-04, -1.1623e-02,  6.1516e-03,  5.2980e-03,\n","          4.0904e-03, -3.2237e-03,  1.3718e-02, -1.8275e-03, -3.7952e-03,\n","         -1.1847e-02, -2.3420e-06,  1.7159e-02,  8.3965e-03,  2.1443e-03,\n","         -9.5324e-03,  1.7593e-02, -1.8119e-02, -1.0295e-02,  2.4196e-04,\n","          1.5599e-02,  4.3085e-03, -5.5552e-03, -7.2795e-03, -5.3302e-03,\n","          6.7372e-03, -6.1675e-03,  1.0393e-02, -5.8226e-03,  6.0743e-03,\n","         -1.1926e-02,  7.4931e-03,  1.6323e-02,  3.3834e-03, -5.1312e-03,\n","         -7.3140e-03, -1.2710e-02,  9.1284e-03, -1.4834e-02, -1.4960e-02,\n","         -1.7480e-02,  8.5691e-03,  5.4853e-03, -8.9314e-03,  1.5910e-02,\n","         -3.3959e-04, -1.6199e-03,  1.3941e-02, -2.5957e-03,  1.6784e-02,\n","         -1.6967e-03, -1.3809e-02,  1.5852e-02, -1.4453e-02,  5.3292e-03,\n","         -6.4283e-03,  1.6150e-04, -3.0931e-03,  5.2978e-03,  1.3896e-02,\n","          9.0107e-03, -4.7015e-03, -1.8979e-02, -4.0687e-03, -1.3915e-02,\n","          6.8543e-03, -2.6004e-03, -1.1072e-02,  4.0479e-03,  3.9728e-03,\n","          2.3824e-03, -1.9303e-02,  8.2788e-03, -2.0900e-03, -4.9244e-03,\n","          2.3196e-02,  4.9706e-03,  7.8066e-03,  1.7297e-03, -3.9635e-03,\n","         -3.0749e-03, -7.2472e-03,  1.2592e-02, -7.8363e-03,  2.2685e-02,\n","         -1.7494e-02,  1.3401e-02, -6.7607e-03, -6.7016e-03, -7.8965e-03,\n","         -3.6355e-03,  1.4366e-02,  2.4411e-03, -3.1453e-03,  2.5094e-02,\n","         -1.5722e-02, -1.5174e-02,  1.8962e-02, -1.4869e-02, -6.0775e-03,\n","          3.1663e-03, -1.3533e-02,  2.4171e-02, -5.2306e-03, -7.7565e-03,\n","         -8.1588e-03, -1.2773e-03,  5.2466e-03,  3.7574e-03, -3.0909e-04,\n","         -1.3759e-03,  1.2835e-02, -3.9369e-03,  1.9361e-02, -3.5121e-03,\n","         -2.3856e-02, -2.8325e-03,  4.7218e-03,  2.0422e-02, -1.1113e-03,\n","         -1.0675e-02,  3.6587e-03, -1.0323e-02, -1.4969e-02, -1.7596e-02,\n","         -1.2283e-02,  4.9182e-04,  7.1819e-03,  4.7240e-03, -1.8446e-03,\n","         -1.7430e-02, -4.2473e-05, -1.9339e-02, -4.3981e-03,  1.2135e-02,\n","          1.5146e-03, -5.6211e-03,  1.3095e-02,  2.5811e-02, -1.5620e-03,\n","          1.3084e-02,  7.5282e-04,  1.5532e-02, -1.5707e-02,  1.2489e-02,\n","          5.7122e-03, -1.3400e-02,  1.3012e-02,  1.4555e-02,  1.2432e-02,\n","          1.2607e-02,  4.3599e-03, -7.9550e-03,  4.1037e-03,  1.3721e-02,\n","         -1.1282e-03,  1.9397e-02, -4.5078e-03, -3.8723e-03, -1.9364e-03,\n","          1.5451e-02, -2.4342e-03,  5.7658e-03,  2.2650e-02,  2.0748e-02,\n","         -9.1603e-03, -6.1797e-03, -1.0622e-02, -7.2928e-03, -2.4264e-02,\n","          1.3958e-02, -1.4133e-02,  1.8517e-02, -5.2048e-03,  6.4145e-03,\n","         -8.5069e-03,  9.2969e-03,  2.6534e-02, -1.5502e-03,  4.4455e-03,\n","         -4.8329e-03,  1.4506e-02,  1.4241e-02, -9.3367e-03, -7.7893e-03,\n","         -8.4133e-03,  7.9845e-03, -1.5542e-03,  5.4295e-03, -1.5205e-02,\n","         -1.1499e-02, -1.7841e-03, -1.1668e-02,  3.8118e-03, -2.1035e-02,\n","          6.5799e-03, -2.5137e-03,  9.2636e-03, -7.0785e-03, -1.4753e-02,\n","          2.2562e-02, -3.8806e-03,  7.5106e-04,  5.1319e-03,  1.2202e-02,\n","          8.5980e-03,  7.2870e-03,  6.9271e-03,  8.1978e-03, -9.5993e-03,\n","          1.9340e-02, -6.5389e-03,  4.1520e-03, -2.2355e-02, -9.3490e-03,\n","         -2.6387e-02,  1.4638e-02, -1.1643e-02, -7.4206e-03,  2.8368e-03,\n","          2.7640e-03, -1.1379e-02,  7.1886e-03,  2.0711e-02,  1.5289e-03,\n","         -7.2895e-03,  7.3367e-04, -9.3448e-03,  1.3246e-02, -1.1819e-02,\n","          8.1108e-03,  1.3110e-02,  1.5439e-02,  1.9400e-02,  9.1024e-03,\n","         -1.6152e-02, -1.2157e-02, -1.8917e-02, -1.1288e-02, -2.0551e-02,\n","         -1.3142e-02, -1.7619e-03, -6.6356e-03,  7.8653e-03, -1.6992e-02,\n","         -1.6047e-02, -1.4465e-02,  8.7523e-03,  3.7092e-03,  9.0864e-04,\n","         -3.3036e-03,  2.6071e-03,  1.7709e-02, -3.7735e-03, -1.2092e-02,\n","         -8.7222e-03, -1.0592e-03, -2.2360e-02,  9.9784e-03, -4.0511e-03,\n","         -8.2204e-03, -4.4903e-03,  1.0732e-02,  1.5016e-02,  4.3270e-03,\n","          6.3696e-04, -6.2102e-03,  7.3538e-03, -1.1557e-02, -1.3677e-02,\n","         -8.9686e-06, -2.3552e-03, -4.4934e-03,  8.3346e-03, -1.7936e-02,\n","          7.6363e-04, -1.8455e-02, -2.0042e-02, -6.2476e-03, -1.7726e-02,\n","         -2.1340e-02, -8.7126e-03, -2.1001e-02, -1.3382e-03,  3.1235e-03,\n","         -1.1397e-03, -1.2864e-02,  8.7099e-03, -6.8206e-03, -1.4356e-02,\n","         -9.5454e-04,  1.3993e-02, -1.3487e-04, -1.0037e-02, -2.6270e-03,\n","         -8.3922e-03,  1.0402e-02, -1.0512e-02,  1.2731e-02, -4.9115e-04,\n","         -1.6644e-02,  2.9233e-03,  1.4450e-02,  9.1422e-03, -4.2596e-03,\n","          9.9713e-03, -8.9641e-03,  1.6372e-02,  1.6297e-02,  4.1699e-03,\n","          1.0040e-02, -6.6477e-03,  1.5532e-02, -1.5607e-02, -2.2937e-03,\n","          1.5737e-02,  1.4737e-03,  8.4276e-03,  4.3916e-04,  1.1182e-02,\n","         -1.3651e-02,  8.8484e-03,  1.3660e-02, -3.6712e-04,  9.0780e-03,\n","          1.4286e-02,  2.0210e-03, -1.7236e-02,  1.8826e-02,  3.3147e-03,\n","         -1.7904e-02,  1.5070e-02, -5.4042e-03,  2.6442e-03,  9.1334e-03,\n","         -4.6901e-03, -4.4026e-03, -7.4686e-03,  1.4452e-02,  7.3946e-03,\n","          5.0920e-03,  5.7654e-03, -1.9087e-03,  3.9057e-03, -1.1128e-02,\n","          4.7670e-03, -3.9227e-03,  1.7867e-02, -3.2862e-03, -1.3791e-02,\n","         -9.8645e-04,  9.8713e-03,  3.8473e-04,  3.0128e-03,  2.7530e-03,\n","          1.2870e-02, -2.3173e-02,  4.4654e-04, -1.9405e-02, -9.2086e-03,\n","         -4.6176e-03,  1.1989e-02, -6.1567e-03, -4.9471e-04,  4.9184e-03,\n","         -1.6113e-02,  1.6959e-02,  1.9358e-02,  8.3011e-03, -1.4332e-02,\n","          4.4184e-03, -1.6218e-02,  1.7238e-02, -2.4484e-02,  1.3397e-02,\n","          1.0772e-02,  3.0818e-04,  2.1662e-02,  9.7259e-03, -1.5723e-02,\n","         -2.5091e-02,  1.0276e-02, -8.8648e-03,  1.0974e-02,  4.5920e-03,\n","          4.8989e-03,  1.0008e-03, -1.1158e-02, -1.7285e-03,  2.1076e-02,\n","         -7.6407e-03, -1.3848e-02, -3.9153e-03, -4.0160e-03,  1.8858e-02,\n","         -1.1279e-02, -1.1863e-02,  1.3433e-02,  2.5245e-03, -2.1389e-03,\n","         -4.6400e-03, -1.0830e-02, -1.9518e-02, -2.1005e-02, -7.4933e-03,\n","         -8.3688e-03,  8.7934e-03, -5.3275e-03, -1.5190e-02, -1.5252e-02,\n","         -7.7186e-03, -1.7213e-02, -6.5344e-04,  1.0166e-02, -3.4023e-03,\n","         -8.3464e-03,  5.8698e-03,  6.3522e-03,  5.6444e-03, -1.6476e-02,\n","          6.3960e-03,  8.9577e-03, -1.1824e-02,  7.2861e-03, -1.1364e-02,\n","         -9.2131e-03,  1.4299e-02,  3.6282e-03, -1.1217e-02,  1.3072e-02,\n","         -6.4432e-03, -1.6081e-02, -2.1382e-02, -2.3643e-03,  1.7626e-03,\n","          3.3426e-03,  1.7076e-02,  1.0561e-02,  1.4635e-02,  4.5990e-04,\n","          2.6896e-03,  2.2150e-02,  2.4315e-02,  1.5517e-02,  2.3470e-02,\n","         -1.8943e-02,  1.3301e-02,  1.6024e-04, -8.8886e-03,  9.2184e-04,\n","         -4.3690e-03, -6.4213e-04,  1.2035e-02,  1.4917e-02,  1.7245e-02]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n"]}],"source":["# AlexNet 실제로 2개의 GPU가 지원되는 경우에 2개의 GPU를 사용하는 방식으로 구현 (multi-GPU 구현)\n","class AlexNet(nn.Module):\n","    def __init__(self):\n","        super(AlexNet, self).__init__()\n","        self.lrn_a = nn.LocalResponseNorm(LRN_N, alpha=LRN_ALPHA, beta=LRN_BETA, k=LRN_K).to('cuda:0')\n","        self.lrn_b = nn.LocalResponseNorm(LRN_N, alpha=LRN_ALPHA, beta=LRN_BETA, k=LRN_K).to('cuda:1')\n","        self.dropout_a = nn.Dropout(DROPOUT).to('cuda:0')\n","        self.dropout_b = nn.Dropout(DROPOUT).to('cuda:1')\n","        self.pool_a = nn.MaxPool2d(kernel_size=3, stride=2).to('cuda:0')\n","        self.pool_b = nn.MaxPool2d(kernel_size=3, stride=2).to('cuda:1')\n","\n","        # Conv 1\n","        self.conv1_a = nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=0).to('cuda:0')\n","        self.conv1_b = nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=0).to('cuda:1')\n","\n","        # Conv 2\n","        self.conv2_a = nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2).to('cuda:0')\n","        self.conv2_b = nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2).to('cuda:1')\n","\n","        # Conv 3\n","        self.conv3_a = nn.Conv2d(128 * 2, 192, kernel_size=3, stride=1, padding=1).to('cuda:0')\n","        self.conv3_b = nn.Conv2d(128 * 2, 192, kernel_size=3, stride=1, padding=1).to('cuda:1')\n","\n","        # Conv 4\n","        self.conv4_a = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1).to('cuda:0')\n","        self.conv4_b = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1).to('cuda:1')\n","\n","        # Conv 5\n","        self.conv5_a = nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1).to('cuda:0')\n","        self.conv5_b = nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1).to('cuda:1')\n","\n","        # FC 6\n","        self.fc6_a = nn.Linear(128 * 6 * 6 * 2, 2048).to('cuda:0')\n","        self.fc6_b = nn.Linear(128 * 6 * 6 * 2, 2048).to('cuda:1')\n","\n","        # FC 7\n","        self.fc7_a = nn.Linear(2048 * 2, 2048).to('cuda:0')\n","        self.fc7_b = nn.Linear(2048 * 2, 2048).to('cuda:1')\n","\n","        # FC 8\n","        self.fc8 = nn.Linear(2048 * 2, NUM_CLASSES).to('cuda:0')\n","\n","    def forward(self, x):\n","        stream0 = torch.cuda.Stream('cuda:0')\n","        stream1 = torch.cuda.Stream('cuda:1')\n","\n","        with torch.cuda.stream(stream0):\n","            x_a = x.to('cuda:0')\n","            # Conv 1\n","            x_a = self.lrn_a(F.relu(self.conv1_a(x_a)))\n","            x_a = self.pool_a(x_a)\n","            # Conv 2\n","            x_a = self.lrn_a(F.relu(self.conv2_a(x_a)))\n","            x_a = self.pool_a(x_a)\n","        with torch.cuda.stream(stream1):\n","            x_b = x.to('cuda:1')\n","            # Conv 1\n","            x_b = self.lrn_b(F.relu(self.conv1_b(x_b)))\n","            x_b = self.pool_b(x_b)\n","            # Conv 2\n","            x_b = self.lrn_b(F.relu(self.conv2_b(x_b)))\n","            x_b = self.pool_b(x_b)\n","\n","        # GPU 통신\n","        stream0.synchronize()\n","        stream1.synchronize()\n","        x_b_ = x_b.to('cuda:0')\n","        x_a_ = x_a.to('cuda:1')\n","\n","        with torch.cuda.stream(stream0):\n","            # Conv 3\n","            x_a = torch.cat((x_a, x_b_), dim=1)\n","            x_a = F.relu(self.conv3_a(x_a))\n","            # Conv 4\n","            x_a = F.relu(self.conv4_a(x_a))\n","            # Conv 5\n","            x_a = F.relu(self.conv5_a(x_a))\n","            x_a = self.pool_a(x_a)\n","            x_a = x_a.view(x_a.size(0), -1)\n","        with torch.cuda.stream(stream1):\n","            # Conv 3\n","            x_b = torch.cat((x_a_, x_b), dim=1)\n","            x_b = F.relu(self.conv3_b(x_b))\n","            # Conv 4\n","            x_b = F.relu(self.conv4_b(x_b))\n","            # Conv 5\n","            x_b = F.relu(self.conv5_b(x_b))\n","            x_b = self.pool_b(x_b)\n","            x_b = x_b.view(x_b.size(0), -1)\n","\n","        # GPU 통신\n","        stream0.synchronize()\n","        stream1.synchronize()\n","        x_b_ = x_b.to('cuda:0')\n","        x_a_ = x_a.to('cuda:1')\n","\n","        with torch.cuda.stream(stream0):\n","            # FC 6, GPU 데이터 합치고 각각 연산 수행\n","            x_a = torch.cat((x_a, x_b_), dim=1)\n","            x_a = self.dropout_a(F.relu(self.fc6_a(x_a)))\n","        with torch.cuda.stream(stream1):\n","            # FC 6, GPU 데이터 합치고 각각 연산 수행\n","            x_b = torch.cat((x_a_, x_b), dim=1)\n","            x_b = self.dropout_b(F.relu(self.fc6_b(x_b)))\n","\n","        # GPU 통신\n","        stream0.synchronize()\n","        stream1.synchronize()\n","        x_b_ = x_b.to('cuda:0')\n","        x_a_ = x_a.to('cuda:1')\n","\n","        with torch.cuda.stream(stream0):\n","            # FC 7, GPU 데이터 합치고 각각 연산 수행\n","            x_a = torch.cat((x_a, x_b_), dim=1)\n","            x_a = self.dropout_a(F.relu(self.fc7_a(x_a)))\n","        with torch.cuda.stream(stream1):\n","            # FC 7, GPU 데이터 합치고 각각 연산 수행\n","            x_b = torch.cat((x_a_, x_b), dim=1)\n","            x_b = self.dropout_b(F.relu(self.fc7_b(x_b)))\n","\n","        stream0.synchronize()\n","        stream1.synchronize()\n","        # FC 8, GPU 데이터 합쳐서 최종 연산\n","        x_b_ = x_b.to('cuda:0')\n","        x = torch.cat((x_a, x_b_), dim=1)\n","        logits = self.fc8(x)\n","\n","        return logits\n","\n","model = AlexNet()\n","\n","# Example input\n","input_size = (1, 3, 227, 227)\n","x = torch.randn(input_size).cuda()\n","\n","# Forward pass\n","output = model(x)\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
